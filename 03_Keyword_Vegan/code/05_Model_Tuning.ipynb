{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/vegan-logo-resized.png\" style=\"float: right; margin: 10px;\">\n",
    "\n",
    "# Model Tuning\n",
    "\n",
    "Author: Gifford Tompkins\n",
    "\n",
    "---\n",
    "\n",
    "Project 03 | Notebook 5 of 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECTIVE \n",
    "In this notebook, we will create a function that will loop through all of our potential models and pull the best hyper-parameters of each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `CountVectorizer`\n",
    "- `TfidVectorizer`\n",
    "- `SVD`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to place these vectorizers into a GridSearchCV, so we will create the grid parameters for the specific vectorizers that we pass.\n",
    "```python\n",
    "CountVectorizer(input='content',\n",
    "    encoding='utf-8',\n",
    "    decode_error='strict',\n",
    "    strip_accents=None,\n",
    "    lowercase=True,\n",
    "    preprocessor=None,\n",
    "    tokenizer=None,\n",
    "    stop_words=None,\n",
    "    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=1.0,\n",
    "    min_df=1,\n",
    "    max_features=None,\n",
    "    vocabulary=None,\n",
    "    binary=False,\n",
    "    dtype=<class 'numpy.int64'>\n",
    ")\n",
    "\n",
    "TfidfVectorizer(\n",
    "    input='content',\n",
    "    encoding='utf-8',\n",
    "    decode_error='strict',\n",
    "    strip_accents=None,\n",
    "    lowercase=True,\n",
    "    preprocessor=None,\n",
    "    tokenizer=None,\n",
    "    analyzer='word',\n",
    "    stop_words=None,\n",
    "    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "    ngram_range=(1, 1),\n",
    "    max_df=1.0,\n",
    "    min_df=1,\n",
    "    max_features=None,\n",
    "    vocabulary=None,\n",
    "    binary=False,\n",
    "    dtype=<class 'numpy.float64'>,\n",
    "    norm='l2',\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=False,\n",
    ")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDA: Singular Value Decomposition\n",
    "```python\n",
    "TruncatedSVD(\n",
    "    n_components=2,\n",
    "    algorithm='randomized',\n",
    "    n_iter=5,\n",
    "    random_state=None,\n",
    "    tol=0.0,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "The list of Estimators we have is also extensive.\n",
    "\n",
    "- Naive Bayes Theorem\n",
    "    - `BinomialNB`\n",
    "    - `MultinomialNB`\n",
    "    - `GaussianNB`\n",
    "- Decision Trees\n",
    "    - `DecisionTreeClassifier`\n",
    "    - `RandomForestClassifier`\n",
    "    - `Extraa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier\n",
    "import numpy as  np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "\n",
    "import time\n",
    "from project_python_code import parse_performance\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_params = {'vec__max_df': [0.80, 0.90, 1.0],\n",
    "    'vec__max_features': [1000, 3000]\n",
    "    'vec__min_df': [3, 5],\n",
    "    'vec__ngram_range': [(1, 1),(1,2)],\n",
    "    'vec__stop_words': [None,'english'],\n",
    "}\n",
    "\n",
    "param_reference = {}\n",
    "\n",
    "# Vectorizer Parameter Dictionaries\n",
    "param_reference[type(CountVectorizer())] = {'name':'vec','parameters':count_vectorizer_params}\n",
    "param_reference[type(TfidfVectorizer())] = {'name':'vec','parameters':tfid_vectorizer_params}\n",
    "\n",
    "\n",
    "# SVD Parameter Dictionaries\n",
    "param_reference[type(TruncatedSVD())] = {'name':'svd',\n",
    "                                         'parameters':{\n",
    "                                             'svd__n_components':[1000, 2000, 3000]}}\n",
    "\n",
    "\n",
    "# Decision Trees, Baggers, Random and Extra Forests\n",
    "param_reference[type(DecisionTreeClassifier())] = {'name':'tree',\n",
    "                                                   'parameters':{\n",
    "                                                       'tree__max_depth': [3, 7],\n",
    "                                                       'tree__min_samples_split': [5, 10],\n",
    "                                                       'tree__min_samples_leaf': [2, 5, 7]}\n",
    "                                                  }\n",
    "\n",
    "param_reference[type(BaggingClassifier())] = {'name':'bagg',\n",
    "                                              'parameters':{\n",
    "                                                  'bagg__base_estimator': [None],\n",
    "                                                  'bagg__n_estimators': [10, 15, 20]}}\n",
    "\n",
    "param_reference[type(RandomForestClassifier())] = {'name':'tree',\n",
    "                                                   'parameters':{\n",
    "                                                       'tree__max_depth': [None, 3, 7, 10],\n",
    "                                                       'tree__min_samples_split': [5, 10, 20],\n",
    "                                                       'tree__min_samples_leaf': [2, 4, 7],\n",
    "                                                       'tree__n_estimators': [6, 8, 10]}}\n",
    "\n",
    "param_reference[type(ExtraTreesClassifier())] = param_reference[type(RandomForestClassifier())]\n",
    "\n",
    "\n",
    "# Adaboosters\n",
    "param_reference[type(AdaBoostClassifier())] = {'name':'boost',\n",
    "                                               'parameters':{\n",
    "                                                   'boost__n_estimators': [45, 50, 55],\n",
    "                                                   'boost__base_estimator__max_depth': [1, 2, 3]}}\n",
    "\n",
    "# param_reference[type(GradientBoostingClassifier())]\n",
    "\n",
    "\n",
    "# Logistic and Linear Model Regressions\n",
    "param_reference[type(LogisticRegression())] = {'name':'logreg',\n",
    "                                               'parameters':{\n",
    "                                                   'logreg__C':np.logspace(-2,2,4)}}\n",
    "\n",
    "param_reference[type(Lasso())] = {'name':'lasso',\n",
    "                                  'parameters':{\n",
    "                                      'lasso__alpha':np.logspace(-2,2,4)}}\n",
    "\n",
    "param_reference[type(RidgeClassifier())] = {'name':'ridge',\n",
    "                                            'parameters':{\n",
    "                                                'ridge__alpha':np.logspace(-2,2,4)}}\n",
    "\n",
    "\n",
    "# Naive Bayes Models\n",
    "param_reference[type(MultinomialNB())] = {'name':'nb',\n",
    "                                          'parameters':{\n",
    "                                              'nb__alpha':np.logspace(-2,2,4)}}\n",
    "\n",
    "param_reference[type(GaussianNB())] = {'name':'nb',\n",
    "                                       'parameters':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_params = {'vec__max_df': [0.8],\n",
    "                             'vec__max_features': [3000],\n",
    "                             'vec__min_df': [5],\n",
    "                             'vec__ngram_range': [(1, 2)],\n",
    "                             'vec__stop_words': ['english']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_vectorizer_params = {'vec__max_df': [0.8],\n",
    "                          'vec__max_features': [3000],\n",
    "                          'vec__min_df': [3],\n",
    "                          'vec__ngram_range': [(1, 2)],\n",
    "                          'vec__stop_words': ['english']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [CountVectorizer(max_df = 0.8,\n",
    "                             max_features = 3000,\n",
    "                             min_df = 5,\n",
    "                             ngram_range = (1, 2),\n",
    "                             stop_words = 'english')\n",
    "                ,TfidfVectorizer(max_df = 0.8,\n",
    "                             max_features = 3000,\n",
    "                             min_df = 3,\n",
    "                             ngram_range = (1, 2),\n",
    "                             stop_words = 'english')\n",
    "#                 ,TruncatedSVD()\n",
    "               ]\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(random_state=42)\n",
    "               ,BaggingClassifier(random_state=42)\n",
    "               ,RandomForestClassifier(random_state=42)\n",
    "               ,ExtraTreesClassifier(random_state=42)\n",
    "               ,AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),random_state=42)\n",
    "               ,LogisticRegression(random_state=42)\n",
    "#                ,Lasso()\n",
    "               ,RidgeClassifier(random_state=42)\n",
    "               ,MultinomialNB()\n",
    "               ,GaussianNB()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'steps': None,\n",
    "              'best_cross_val': None,\n",
    "              'best_params': None,\n",
    "              'train_score': None,\n",
    "              'test_score': None,\n",
    "              'sensitivity': None,\n",
    "              'specificity': None,\n",
    "              'confusion_matrix': None,\n",
    "              'runtime': None},\n",
    "             index=[0]\n",
    "            ).to_csv('../data/model_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condenser(sparse_matrix):\n",
    "    return sparse_matrix.toarray()\n",
    "\n",
    "condenser = FunctionTransformer(condenser, accept_sparse=True, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(transformer, classifier, param_reference=param_reference):\n",
    "    pipeline = Pipeline([\n",
    "        ('vec',transformer),\n",
    "        ('condenser', condenser),\n",
    "        (param_reference[type(classifier)]['name'],classifier)\n",
    "    ])\n",
    "    print(f\"Pipeline steps: {pipeline.steps}\")\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gridsearch(transformer, classifier, param_dict=param_reference):\n",
    "    pipe_params = param_dict[type(classifier)]['parameters']\n",
    "    \n",
    "#     print(pipe_params)\n",
    "    \n",
    "    pipeline = build_pipeline(transformer, classifier, param_dict)\n",
    "    \n",
    "    gridsearch = GridSearchCV(estimator=pipeline,\n",
    "                              param_grid=pipe_params,\n",
    "                              scoring='accuracy',\n",
    "                              verbose=4,\n",
    "                              n_jobs=4,\n",
    "                              cv=3)\n",
    "    return gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluate_grid(grid, X_train, X_test, y_train, y_test):\n",
    "    model = {}\n",
    "    t_0 = time.time()\n",
    "    print(f\"Fitting model: {t_0}\")\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(f\"Model fit: {time.time()-t_0}\")\n",
    "    \n",
    "    model['steps'] = [type(step) for _,step in grid.estimator.steps]\n",
    "    model['best_cross_val'] = grid.best_score_\n",
    "    model['best_params'] = grid.best_params_\n",
    "    model['train_score'] = grid.score(X_train, y_train)\n",
    "    model['test_score'] = grid.score(X_test, y_test)\n",
    "    model['sensitivity'],model['specificity'],model['confusion_matrix'] = parse_performance(grid, X_test, y_test)\n",
    "    model['runtime'] = time.time() - t_0\n",
    "    \n",
    "#     with open('../data/model_scores.csv', 'a') as f:\n",
    "#         model.to_csv(f, header=False, index=False)\n",
    "#     print(f\"Model with Testing Score: {model['test_score']} appended to model scores.\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['vegan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10931,)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps: [('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=5,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'))]\n",
      "\n",
      "Created grid: {'tree__max_depth': [3, 7], 'tree__min_samples_split': [5, 10], 'tree__min_samples_leaf': [2, 5, 7]}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571364701.789951\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=4)]: Done  36 out of  36 | elapsed:   37.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 42.90650486946106\n",
      "Model fit after: 43.94465708732605 seconds.\n",
      "Pipeline steps: [('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=5,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('bagg', BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=42,\n",
      "         verbose=0, warm_start=False))]\n",
      "\n",
      "Created grid: {'bagg__base_estimator': [None], 'bagg__n_estimators': [10, 15, 20]}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571364745.756635\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   9 | elapsed:  2.0min remaining:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done   9 out of   9 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 316.94268894195557\n",
      "Model fit after: 330.54615592956543 seconds.\n",
      "Pipeline steps: [('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=5,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('tree', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False))]\n",
      "\n",
      "Created grid: {'tree__max_depth': [None, 3, 7, 10], 'tree__min_samples_split': [5, 10, 20], 'tree__min_samples_leaf': [2, 4, 7], 'tree__n_estimators': [6, 8, 10]}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571365076.327858\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done 213 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=4)]: Done 324 out of 324 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 203.439129114151\n",
      "Model fit after: 204.6124243736267 seconds.\n",
      "Pipeline steps: [('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=5,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('tree', ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "           oob_score=False, random_state=42, verbose=0, warm_start=False))]\n",
      "\n",
      "Created grid: {'tree__max_depth': [None, 3, 7, 10], 'tree__min_samples_split': [5, 10, 20], 'tree__min_samples_leaf': [2, 4, 7], 'tree__n_estimators': [6, 8, 10]}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571365280.961944\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   43.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=4)]: Done 213 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=4)]: Done 324 out of 324 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 295.1760609149933\n",
      "Model fit after: 296.3481879234314 seconds.\n",
      "Pipeline steps: [('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=5,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('boost', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=42))]\n",
      "\n",
      "Created grid: {'boost__n_estimators': [45, 50, 55], 'boost__base_estimator__max_depth': [1, 2, 3]}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571365577.3322308\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=4)]: Done  27 out of  27 | elapsed:  7.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  27 out of  27 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 554.1015543937683\n",
      "Model fit after: 555.8675911426544 seconds.\n",
      "Pipeline steps: [('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=5,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('logreg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]\n",
      "\n",
      "Created grid: {'logreg__C': array([1.00000000e-02, 2.15443469e-01, 4.64158883e+00, 1.00000000e+02])}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571366133.221942\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   9 out of  12 | elapsed:    6.9s remaining:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:    7.4s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 8.986255884170532\n",
      "Model fit after: 10.185841083526611 seconds.\n",
      "Pipeline steps: [('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=5,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('ridge', RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=42, solver='auto',\n",
      "        tol=0.001))]\n",
      "\n",
      "Created grid: {'ridge__alpha': array([1.00000000e-02, 2.15443469e-01, 4.64158883e+00, 1.00000000e+02])}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571366143.427486\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   9 out of  12 | elapsed:   15.4s remaining:    5.1s\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:   15.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 18.00074601173401\n",
      "Model fit after: 19.333905935287476 seconds.\n",
      "Pipeline steps: [('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=5,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]\n",
      "\n",
      "Created grid: {'nb__alpha': array([1.00000000e-02, 2.15443469e-01, 4.64158883e+00, 1.00000000e+02])}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571366162.790238\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   9 out of  12 | elapsed:    7.4s remaining:    2.5s\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:    7.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 8.737454175949097\n",
      "Model fit after: 10.095630884170532 seconds.\n",
      "Pipeline steps: [('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=5,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('nb', GaussianNB(priors=None, var_smoothing=1e-09))]\n",
      "\n",
      "Created grid: {}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571366172.906958\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    3.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 4.835395812988281\n",
      "Model fit after: 6.8293821811676025 seconds.\n",
      "Pipeline steps: [('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=3,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'))]\n",
      "\n",
      "Created grid: {'tree__max_depth': [3, 7], 'tree__min_samples_split': [5, 10], 'tree__min_samples_leaf': [2, 5, 7]}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571366179.7654378\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=4)]: Done  36 out of  36 | elapsed:   36.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 42.13371801376343\n",
      "Model fit after: 43.18635892868042 seconds.\n",
      "Pipeline steps: [('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=3,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('bagg', BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=42,\n",
      "         verbose=0, warm_start=False))]\n",
      "\n",
      "Created grid: {'bagg__base_estimator': [None], 'bagg__n_estimators': [10, 15, 20]}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571366222.973909\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   9 | elapsed:  2.5min remaining:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done   9 out of   9 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 418.8007869720459\n",
      "Model fit after: 432.46011304855347 seconds.\n",
      "Pipeline steps: [('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=3,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('tree', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False))]\n",
      "\n",
      "Created grid: {'tree__max_depth': [None, 3, 7, 10], 'tree__min_samples_split': [5, 10, 20], 'tree__min_samples_leaf': [2, 4, 7], 'tree__n_estimators': [6, 8, 10]}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571366655.4583821\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done 213 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done 324 out of 324 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 206.09593987464905\n",
      "Model fit after: 207.2550449371338 seconds.\n",
      "Pipeline steps: [('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=3,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('tree', ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "           oob_score=False, random_state=42, verbose=0, warm_start=False))]\n",
      "\n",
      "Created grid: {'tree__max_depth': [None, 3, 7, 10], 'tree__min_samples_split': [5, 10, 20], 'tree__min_samples_leaf': [2, 4, 7], 'tree__n_estimators': [6, 8, 10]}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571366862.73539\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   44.2s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=4)]: Done 213 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=4)]: Done 324 out of 324 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 300.5494351387024\n",
      "Model fit after: 301.746150970459 seconds.\n",
      "Pipeline steps: [('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=3,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('boost', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=42))]\n",
      "\n",
      "Created grid: {'boost__n_estimators': [45, 50, 55], 'boost__base_estimator__max_depth': [1, 2, 3]}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571367164.50333\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=4)]: Done  27 out of  27 | elapsed:  7.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  27 out of  27 | elapsed:  7.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 525.0438668727875\n",
      "Model fit after: 526.7477450370789 seconds.\n",
      "Pipeline steps: [('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=3,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('logreg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]\n",
      "\n",
      "Created grid: {'logreg__C': array([1.00000000e-02, 2.15443469e-01, 4.64158883e+00, 1.00000000e+02])}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571367691.271827\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   9 out of  12 | elapsed:    5.4s remaining:    1.8s\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:    5.6s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 6.781074047088623\n",
      "Model fit after: 7.8101513385772705 seconds.\n",
      "Pipeline steps: [('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=3,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('ridge', RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=42, solver='auto',\n",
      "        tol=0.001))]\n",
      "\n",
      "Created grid: {'ridge__alpha': array([1.00000000e-02, 2.15443469e-01, 4.64158883e+00, 1.00000000e+02])}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571367699.096595\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   9 out of  12 | elapsed:   14.0s remaining:    4.7s\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:   14.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 16.4378719329834\n",
      "Model fit after: 17.614206075668335 seconds.\n",
      "Pipeline steps: [('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=3,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]\n",
      "\n",
      "Created grid: {'nb__alpha': array([1.00000000e-02, 2.15443469e-01, 4.64158883e+00, 1.00000000e+02])}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571367716.725877\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   9 out of  12 | elapsed:    6.6s remaining:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:    6.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 7.900869131088257\n",
      "Model fit after: 9.061836004257202 seconds.\n",
      "Pipeline steps: [('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.8, max_features=3000, min_df=3,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('condenser', FunctionTransformer(accept_sparse=True, check_inverse=True,\n",
      "          func=<function condenser at 0x1a2deea8c8>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
      "          validate=False)), ('nb', GaussianNB(priors=None, var_smoothing=1e-09))]\n",
      "\n",
      "Created grid: {}\n",
      "Begin fitting and evaluation.\n",
      "Fitting model: 1571367725.802698\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    3.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: 4.797494173049927\n",
      "Model fit after: 6.832221031188965 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>best_cross_val</th>\n",
       "      <th>best_params</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   steps  best_cross_val  best_params  train_score  test_score  sensitivity  \\\n",
       "0    NaN             NaN          NaN          NaN         NaN          NaN   \n",
       "\n",
       "   specificity  confusion_matrix  runtime  \n",
       "0          NaN               NaN      NaN  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for transformer in transformers:\n",
    "    for classifier in classifiers:\n",
    "        \n",
    "        grid = build_gridsearch(transformer, classifier, param_reference)\n",
    "        print(f\"\\nCreated grid: {grid.param_grid}\")\n",
    "        \n",
    "        t_0 = time.time()\n",
    "        print(\"Begin fitting and evaluation.\")\n",
    "        models.append(run_evaluate_grid(grid, X_train, X_test, y_train, y_test))\n",
    "        print(f\"Model fit after: {time.time() - t_0} seconds.\")\n",
    "        \n",
    "        \n",
    "        \n",
    "pd.DataFrame(models)\n",
    "\n",
    "pd.read_csv('../data/model_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('condenser', condenser),\n",
    "    ('tree', DecisionTreeClassifier())\n",
    "]),param_grid={\n",
    "    **param_reference[type(CountVectorizer())]['parameters'],\n",
    "    **param_reference[type(DecisionTreeClassifier())]['parameters']\n",
    "},\n",
    "                    verbose=3,\n",
    "                    n_jobs=3,\n",
    "                    cv=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vec__stop_words': [None, 'english'],\n",
       " 'vec__ngram_range': [(1, 1), (1, 2)],\n",
       " 'vec__max_df': [0.8, 0.9, 1.0],\n",
       " 'vec__min_df': [3, 5],\n",
       " 'vec__max_features': [1000, 3000],\n",
       " 'tree__random_state': [42]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=3)]: Done 144 out of 144 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'))]),\n",
       "       fit_params=None, iid='warn', n_jobs=3,\n",
       "       param_grid={'vec__stop_words': [None, 'english'], 'vec__ngram_range': [(1, 1), (1, 2)], 'vec__max_df': [0.8, 0.9, 1.0], 'vec__min_df': [3, 5], 'vec__max_features': [1000, 3000], 'tree__random_state': [42]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.282081695397695\n"
     ]
    }
   ],
   "source": [
    "print((time.time() - t_0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tree__random_state': 42,\n",
       " 'vec__max_df': 0.8,\n",
       " 'vec__max_features': 3000,\n",
       " 'vec__min_df': 3,\n",
       " 'vec__ngram_range': (1, 2),\n",
       " 'vec__stop_words': 'english'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_reference[type(CountVectorizer())] = count_vectorizer_params\n",
    "param_reference[type(TfidfVectorizer())] = tfid_vectorizer_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvec = CountVectorizer()\n",
    "# X_train_cv = cvec.fit_transform(X_train)\n",
    "\n",
    "# X_train_cv\n",
    "\n",
    "# X_train_cv.toarray()\n",
    "\n",
    "# X_train\n",
    "\n",
    "# logreg = LogisticRegression()\n",
    "\n",
    "# transformers\n",
    "\n",
    "# classifiers\n",
    "\n",
    "# t_0 = time.time()\n",
    "# tfid = TfidfVectorizer()\n",
    "# X_train_vec = tfid.fit_transform(X_train)\n",
    "# X_test_vec = tfid.transform(X_test)\n",
    "\n",
    "# gs = GridSearchCV(DecisionTreeClassifier(),\n",
    "#                   {'max_depth': [None, 3, 7, 10],\n",
    "#                    'min_samples_split': [5, 10, 20],\n",
    "#                    'min_samples_leaf': [2, 4, 7],\n",
    "#                    'random_state': [42]},\n",
    "#                   cv=5\n",
    "#                  )\n",
    "\n",
    "# gs.fit(X_train_vec,y_train)\n",
    "# print(time.time()-t_0)\n",
    "\n",
    "# param_reference[type(DecisionTreeClassifier())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.to_csv('../data/model_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>best_cross_val</th>\n",
       "      <th>best_params</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.723447</td>\n",
       "      <td>{'tree__max_depth': 7, 'tree__min_samples_leaf...</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.720088</td>\n",
       "      <td>0.940437</td>\n",
       "      <td>0.497795</td>\n",
       "      <td>(903, 911, 109, 1721)</td>\n",
       "      <td>43.944631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.770927</td>\n",
       "      <td>{'bagg__base_estimator': None, 'bagg__n_estima...</td>\n",
       "      <td>0.974476</td>\n",
       "      <td>0.760154</td>\n",
       "      <td>0.740984</td>\n",
       "      <td>0.779493</td>\n",
       "      <td>(1414, 400, 474, 1356)</td>\n",
       "      <td>330.546137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.786662</td>\n",
       "      <td>{'tree__max_depth': None, 'tree__min_samples_l...</td>\n",
       "      <td>0.864056</td>\n",
       "      <td>0.777168</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.775634</td>\n",
       "      <td>(1407, 407, 405, 1425)</td>\n",
       "      <td>204.612410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.782454</td>\n",
       "      <td>{'tree__max_depth': None, 'tree__min_samples_l...</td>\n",
       "      <td>0.844113</td>\n",
       "      <td>0.761526</td>\n",
       "      <td>0.748087</td>\n",
       "      <td>0.775083</td>\n",
       "      <td>(1406, 408, 461, 1369)</td>\n",
       "      <td>296.348172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.770286</td>\n",
       "      <td>{'boost__base_estimator__max_depth': 2, 'boost...</td>\n",
       "      <td>0.805050</td>\n",
       "      <td>0.766740</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.699559</td>\n",
       "      <td>(1269, 545, 305, 1525)</td>\n",
       "      <td>555.867576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.788949</td>\n",
       "      <td>{'logreg__C': 0.01}</td>\n",
       "      <td>0.807520</td>\n",
       "      <td>0.787322</td>\n",
       "      <td>0.853005</td>\n",
       "      <td>0.721058</td>\n",
       "      <td>(1308, 506, 269, 1561)</td>\n",
       "      <td>10.185824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.777239</td>\n",
       "      <td>{'ridge__alpha': 100.0}</td>\n",
       "      <td>0.820968</td>\n",
       "      <td>0.778540</td>\n",
       "      <td>0.863388</td>\n",
       "      <td>0.692944</td>\n",
       "      <td>(1257, 557, 250, 1580)</td>\n",
       "      <td>19.333888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.752447</td>\n",
       "      <td>{'nb__alpha': 4.6415888336127775}</td>\n",
       "      <td>0.773488</td>\n",
       "      <td>0.749726</td>\n",
       "      <td>0.692896</td>\n",
       "      <td>0.807056</td>\n",
       "      <td>(1464, 350, 562, 1268)</td>\n",
       "      <td>10.095613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.688592</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.733693</td>\n",
       "      <td>0.705543</td>\n",
       "      <td>0.831694</td>\n",
       "      <td>0.578280</td>\n",
       "      <td>(1049, 765, 308, 1522)</td>\n",
       "      <td>6.829363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.720520</td>\n",
       "      <td>{'tree__max_depth': 7, 'tree__min_samples_leaf...</td>\n",
       "      <td>0.737718</td>\n",
       "      <td>0.722558</td>\n",
       "      <td>0.942077</td>\n",
       "      <td>0.501103</td>\n",
       "      <td>(909, 905, 106, 1724)</td>\n",
       "      <td>43.186298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.781539</td>\n",
       "      <td>{'bagg__base_estimator': None, 'bagg__n_estima...</td>\n",
       "      <td>0.975025</td>\n",
       "      <td>0.773875</td>\n",
       "      <td>0.772678</td>\n",
       "      <td>0.775083</td>\n",
       "      <td>(1406, 408, 416, 1414)</td>\n",
       "      <td>432.460099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.782271</td>\n",
       "      <td>{'tree__max_depth': None, 'tree__min_samples_l...</td>\n",
       "      <td>0.850700</td>\n",
       "      <td>0.772503</td>\n",
       "      <td>0.781421</td>\n",
       "      <td>0.763506</td>\n",
       "      <td>(1385, 429, 400, 1430)</td>\n",
       "      <td>207.255031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.783368</td>\n",
       "      <td>{'tree__max_depth': None, 'tree__min_samples_l...</td>\n",
       "      <td>0.835422</td>\n",
       "      <td>0.765917</td>\n",
       "      <td>0.757377</td>\n",
       "      <td>0.774531</td>\n",
       "      <td>(1405, 409, 444, 1386)</td>\n",
       "      <td>301.746136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.772390</td>\n",
       "      <td>{'boost__base_estimator__max_depth': 2, 'boost...</td>\n",
       "      <td>0.807428</td>\n",
       "      <td>0.767838</td>\n",
       "      <td>0.820765</td>\n",
       "      <td>0.714443</td>\n",
       "      <td>(1296, 518, 328, 1502)</td>\n",
       "      <td>526.747730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.795261</td>\n",
       "      <td>{'logreg__C': 0.21544346900318834}</td>\n",
       "      <td>0.821974</td>\n",
       "      <td>0.787870</td>\n",
       "      <td>0.813661</td>\n",
       "      <td>0.761852</td>\n",
       "      <td>(1382, 432, 341, 1489)</td>\n",
       "      <td>7.810136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.789955</td>\n",
       "      <td>{'ridge__alpha': 100.0}</td>\n",
       "      <td>0.804592</td>\n",
       "      <td>0.781010</td>\n",
       "      <td>0.796175</td>\n",
       "      <td>0.765711</td>\n",
       "      <td>(1389, 425, 373, 1457)</td>\n",
       "      <td>17.614181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.754643</td>\n",
       "      <td>{'nb__alpha': 4.6415888336127775}</td>\n",
       "      <td>0.789132</td>\n",
       "      <td>0.753019</td>\n",
       "      <td>0.695082</td>\n",
       "      <td>0.811466</td>\n",
       "      <td>(1472, 342, 558, 1272)</td>\n",
       "      <td>9.061816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.710456</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.812917</td>\n",
       "      <td>0.732986</td>\n",
       "      <td>0.740437</td>\n",
       "      <td>0.725469</td>\n",
       "      <td>(1316, 498, 475, 1355)</td>\n",
       "      <td>6.832202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                steps  best_cross_val  \\\n",
       "0   [<class 'sklearn.feature_extraction.text.Count...        0.723447   \n",
       "1   [<class 'sklearn.feature_extraction.text.Count...        0.770927   \n",
       "2   [<class 'sklearn.feature_extraction.text.Count...        0.786662   \n",
       "3   [<class 'sklearn.feature_extraction.text.Count...        0.782454   \n",
       "4   [<class 'sklearn.feature_extraction.text.Count...        0.770286   \n",
       "5   [<class 'sklearn.feature_extraction.text.Count...        0.788949   \n",
       "6   [<class 'sklearn.feature_extraction.text.Count...        0.777239   \n",
       "7   [<class 'sklearn.feature_extraction.text.Count...        0.752447   \n",
       "8   [<class 'sklearn.feature_extraction.text.Count...        0.688592   \n",
       "9   [<class 'sklearn.feature_extraction.text.Tfidf...        0.720520   \n",
       "10  [<class 'sklearn.feature_extraction.text.Tfidf...        0.781539   \n",
       "11  [<class 'sklearn.feature_extraction.text.Tfidf...        0.782271   \n",
       "12  [<class 'sklearn.feature_extraction.text.Tfidf...        0.783368   \n",
       "13  [<class 'sklearn.feature_extraction.text.Tfidf...        0.772390   \n",
       "14  [<class 'sklearn.feature_extraction.text.Tfidf...        0.795261   \n",
       "15  [<class 'sklearn.feature_extraction.text.Tfidf...        0.789955   \n",
       "16  [<class 'sklearn.feature_extraction.text.Tfidf...        0.754643   \n",
       "17  [<class 'sklearn.feature_extraction.text.Tfidf...        0.710456   \n",
       "\n",
       "                                          best_params  train_score  \\\n",
       "0   {'tree__max_depth': 7, 'tree__min_samples_leaf...     0.732961   \n",
       "1   {'bagg__base_estimator': None, 'bagg__n_estima...     0.974476   \n",
       "2   {'tree__max_depth': None, 'tree__min_samples_l...     0.864056   \n",
       "3   {'tree__max_depth': None, 'tree__min_samples_l...     0.844113   \n",
       "4   {'boost__base_estimator__max_depth': 2, 'boost...     0.805050   \n",
       "5                                 {'logreg__C': 0.01}     0.807520   \n",
       "6                             {'ridge__alpha': 100.0}     0.820968   \n",
       "7                   {'nb__alpha': 4.6415888336127775}     0.773488   \n",
       "8                                                  {}     0.733693   \n",
       "9   {'tree__max_depth': 7, 'tree__min_samples_leaf...     0.737718   \n",
       "10  {'bagg__base_estimator': None, 'bagg__n_estima...     0.975025   \n",
       "11  {'tree__max_depth': None, 'tree__min_samples_l...     0.850700   \n",
       "12  {'tree__max_depth': None, 'tree__min_samples_l...     0.835422   \n",
       "13  {'boost__base_estimator__max_depth': 2, 'boost...     0.807428   \n",
       "14                 {'logreg__C': 0.21544346900318834}     0.821974   \n",
       "15                            {'ridge__alpha': 100.0}     0.804592   \n",
       "16                  {'nb__alpha': 4.6415888336127775}     0.789132   \n",
       "17                                                 {}     0.812917   \n",
       "\n",
       "    test_score  sensitivity  specificity        confusion_matrix     runtime  \n",
       "0     0.720088     0.940437     0.497795   (903, 911, 109, 1721)   43.944631  \n",
       "1     0.760154     0.740984     0.779493  (1414, 400, 474, 1356)  330.546137  \n",
       "2     0.777168     0.778689     0.775634  (1407, 407, 405, 1425)  204.612410  \n",
       "3     0.761526     0.748087     0.775083  (1406, 408, 461, 1369)  296.348172  \n",
       "4     0.766740     0.833333     0.699559  (1269, 545, 305, 1525)  555.867576  \n",
       "5     0.787322     0.853005     0.721058  (1308, 506, 269, 1561)   10.185824  \n",
       "6     0.778540     0.863388     0.692944  (1257, 557, 250, 1580)   19.333888  \n",
       "7     0.749726     0.692896     0.807056  (1464, 350, 562, 1268)   10.095613  \n",
       "8     0.705543     0.831694     0.578280  (1049, 765, 308, 1522)    6.829363  \n",
       "9     0.722558     0.942077     0.501103   (909, 905, 106, 1724)   43.186298  \n",
       "10    0.773875     0.772678     0.775083  (1406, 408, 416, 1414)  432.460099  \n",
       "11    0.772503     0.781421     0.763506  (1385, 429, 400, 1430)  207.255031  \n",
       "12    0.765917     0.757377     0.774531  (1405, 409, 444, 1386)  301.746136  \n",
       "13    0.767838     0.820765     0.714443  (1296, 518, 328, 1502)  526.747730  \n",
       "14    0.787870     0.813661     0.761852  (1382, 432, 341, 1489)    7.810136  \n",
       "15    0.781010     0.796175     0.765711  (1389, 425, 373, 1457)   17.614181  \n",
       "16    0.753019     0.695082     0.811466  (1472, 342, 558, 1272)    9.061816  \n",
       "17    0.732986     0.740437     0.725469  (1316, 498, 475, 1355)    6.832202  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>best_cross_val</th>\n",
       "      <th>best_params</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.795261</td>\n",
       "      <td>{'logreg__C': 0.21544346900318834}</td>\n",
       "      <td>0.821974</td>\n",
       "      <td>0.787870</td>\n",
       "      <td>0.813661</td>\n",
       "      <td>0.761852</td>\n",
       "      <td>(1382, 432, 341, 1489)</td>\n",
       "      <td>7.810136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.788949</td>\n",
       "      <td>{'logreg__C': 0.01}</td>\n",
       "      <td>0.807520</td>\n",
       "      <td>0.787322</td>\n",
       "      <td>0.853005</td>\n",
       "      <td>0.721058</td>\n",
       "      <td>(1308, 506, 269, 1561)</td>\n",
       "      <td>10.185824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.789955</td>\n",
       "      <td>{'ridge__alpha': 100.0}</td>\n",
       "      <td>0.804592</td>\n",
       "      <td>0.781010</td>\n",
       "      <td>0.796175</td>\n",
       "      <td>0.765711</td>\n",
       "      <td>(1389, 425, 373, 1457)</td>\n",
       "      <td>17.614181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.777239</td>\n",
       "      <td>{'ridge__alpha': 100.0}</td>\n",
       "      <td>0.820968</td>\n",
       "      <td>0.778540</td>\n",
       "      <td>0.863388</td>\n",
       "      <td>0.692944</td>\n",
       "      <td>(1257, 557, 250, 1580)</td>\n",
       "      <td>19.333888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.786662</td>\n",
       "      <td>{'tree__max_depth': None, 'tree__min_samples_l...</td>\n",
       "      <td>0.864056</td>\n",
       "      <td>0.777168</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.775634</td>\n",
       "      <td>(1407, 407, 405, 1425)</td>\n",
       "      <td>204.612410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.781539</td>\n",
       "      <td>{'bagg__base_estimator': None, 'bagg__n_estima...</td>\n",
       "      <td>0.975025</td>\n",
       "      <td>0.773875</td>\n",
       "      <td>0.772678</td>\n",
       "      <td>0.775083</td>\n",
       "      <td>(1406, 408, 416, 1414)</td>\n",
       "      <td>432.460099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.782271</td>\n",
       "      <td>{'tree__max_depth': None, 'tree__min_samples_l...</td>\n",
       "      <td>0.850700</td>\n",
       "      <td>0.772503</td>\n",
       "      <td>0.781421</td>\n",
       "      <td>0.763506</td>\n",
       "      <td>(1385, 429, 400, 1430)</td>\n",
       "      <td>207.255031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.772390</td>\n",
       "      <td>{'boost__base_estimator__max_depth': 2, 'boost...</td>\n",
       "      <td>0.807428</td>\n",
       "      <td>0.767838</td>\n",
       "      <td>0.820765</td>\n",
       "      <td>0.714443</td>\n",
       "      <td>(1296, 518, 328, 1502)</td>\n",
       "      <td>526.747730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.770286</td>\n",
       "      <td>{'boost__base_estimator__max_depth': 2, 'boost...</td>\n",
       "      <td>0.805050</td>\n",
       "      <td>0.766740</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.699559</td>\n",
       "      <td>(1269, 545, 305, 1525)</td>\n",
       "      <td>555.867576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.783368</td>\n",
       "      <td>{'tree__max_depth': None, 'tree__min_samples_l...</td>\n",
       "      <td>0.835422</td>\n",
       "      <td>0.765917</td>\n",
       "      <td>0.757377</td>\n",
       "      <td>0.774531</td>\n",
       "      <td>(1405, 409, 444, 1386)</td>\n",
       "      <td>301.746136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.782454</td>\n",
       "      <td>{'tree__max_depth': None, 'tree__min_samples_l...</td>\n",
       "      <td>0.844113</td>\n",
       "      <td>0.761526</td>\n",
       "      <td>0.748087</td>\n",
       "      <td>0.775083</td>\n",
       "      <td>(1406, 408, 461, 1369)</td>\n",
       "      <td>296.348172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.770927</td>\n",
       "      <td>{'bagg__base_estimator': None, 'bagg__n_estima...</td>\n",
       "      <td>0.974476</td>\n",
       "      <td>0.760154</td>\n",
       "      <td>0.740984</td>\n",
       "      <td>0.779493</td>\n",
       "      <td>(1414, 400, 474, 1356)</td>\n",
       "      <td>330.546137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.754643</td>\n",
       "      <td>{'nb__alpha': 4.6415888336127775}</td>\n",
       "      <td>0.789132</td>\n",
       "      <td>0.753019</td>\n",
       "      <td>0.695082</td>\n",
       "      <td>0.811466</td>\n",
       "      <td>(1472, 342, 558, 1272)</td>\n",
       "      <td>9.061816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.752447</td>\n",
       "      <td>{'nb__alpha': 4.6415888336127775}</td>\n",
       "      <td>0.773488</td>\n",
       "      <td>0.749726</td>\n",
       "      <td>0.692896</td>\n",
       "      <td>0.807056</td>\n",
       "      <td>(1464, 350, 562, 1268)</td>\n",
       "      <td>10.095613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.710456</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.812917</td>\n",
       "      <td>0.732986</td>\n",
       "      <td>0.740437</td>\n",
       "      <td>0.725469</td>\n",
       "      <td>(1316, 498, 475, 1355)</td>\n",
       "      <td>6.832202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Tfidf...</td>\n",
       "      <td>0.720520</td>\n",
       "      <td>{'tree__max_depth': 7, 'tree__min_samples_leaf...</td>\n",
       "      <td>0.737718</td>\n",
       "      <td>0.722558</td>\n",
       "      <td>0.942077</td>\n",
       "      <td>0.501103</td>\n",
       "      <td>(909, 905, 106, 1724)</td>\n",
       "      <td>43.186298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.723447</td>\n",
       "      <td>{'tree__max_depth': 7, 'tree__min_samples_leaf...</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.720088</td>\n",
       "      <td>0.940437</td>\n",
       "      <td>0.497795</td>\n",
       "      <td>(903, 911, 109, 1721)</td>\n",
       "      <td>43.944631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>[&lt;class 'sklearn.feature_extraction.text.Count...</td>\n",
       "      <td>0.688592</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.733693</td>\n",
       "      <td>0.705543</td>\n",
       "      <td>0.831694</td>\n",
       "      <td>0.578280</td>\n",
       "      <td>(1049, 765, 308, 1522)</td>\n",
       "      <td>6.829363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                steps  best_cross_val  \\\n",
       "14  [<class 'sklearn.feature_extraction.text.Tfidf...        0.795261   \n",
       "5   [<class 'sklearn.feature_extraction.text.Count...        0.788949   \n",
       "15  [<class 'sklearn.feature_extraction.text.Tfidf...        0.789955   \n",
       "6   [<class 'sklearn.feature_extraction.text.Count...        0.777239   \n",
       "2   [<class 'sklearn.feature_extraction.text.Count...        0.786662   \n",
       "10  [<class 'sklearn.feature_extraction.text.Tfidf...        0.781539   \n",
       "11  [<class 'sklearn.feature_extraction.text.Tfidf...        0.782271   \n",
       "13  [<class 'sklearn.feature_extraction.text.Tfidf...        0.772390   \n",
       "4   [<class 'sklearn.feature_extraction.text.Count...        0.770286   \n",
       "12  [<class 'sklearn.feature_extraction.text.Tfidf...        0.783368   \n",
       "3   [<class 'sklearn.feature_extraction.text.Count...        0.782454   \n",
       "1   [<class 'sklearn.feature_extraction.text.Count...        0.770927   \n",
       "16  [<class 'sklearn.feature_extraction.text.Tfidf...        0.754643   \n",
       "7   [<class 'sklearn.feature_extraction.text.Count...        0.752447   \n",
       "17  [<class 'sklearn.feature_extraction.text.Tfidf...        0.710456   \n",
       "9   [<class 'sklearn.feature_extraction.text.Tfidf...        0.720520   \n",
       "0   [<class 'sklearn.feature_extraction.text.Count...        0.723447   \n",
       "8   [<class 'sklearn.feature_extraction.text.Count...        0.688592   \n",
       "\n",
       "                                          best_params  train_score  \\\n",
       "14                 {'logreg__C': 0.21544346900318834}     0.821974   \n",
       "5                                 {'logreg__C': 0.01}     0.807520   \n",
       "15                            {'ridge__alpha': 100.0}     0.804592   \n",
       "6                             {'ridge__alpha': 100.0}     0.820968   \n",
       "2   {'tree__max_depth': None, 'tree__min_samples_l...     0.864056   \n",
       "10  {'bagg__base_estimator': None, 'bagg__n_estima...     0.975025   \n",
       "11  {'tree__max_depth': None, 'tree__min_samples_l...     0.850700   \n",
       "13  {'boost__base_estimator__max_depth': 2, 'boost...     0.807428   \n",
       "4   {'boost__base_estimator__max_depth': 2, 'boost...     0.805050   \n",
       "12  {'tree__max_depth': None, 'tree__min_samples_l...     0.835422   \n",
       "3   {'tree__max_depth': None, 'tree__min_samples_l...     0.844113   \n",
       "1   {'bagg__base_estimator': None, 'bagg__n_estima...     0.974476   \n",
       "16                  {'nb__alpha': 4.6415888336127775}     0.789132   \n",
       "7                   {'nb__alpha': 4.6415888336127775}     0.773488   \n",
       "17                                                 {}     0.812917   \n",
       "9   {'tree__max_depth': 7, 'tree__min_samples_leaf...     0.737718   \n",
       "0   {'tree__max_depth': 7, 'tree__min_samples_leaf...     0.732961   \n",
       "8                                                  {}     0.733693   \n",
       "\n",
       "    test_score  sensitivity  specificity        confusion_matrix     runtime  \n",
       "14    0.787870     0.813661     0.761852  (1382, 432, 341, 1489)    7.810136  \n",
       "5     0.787322     0.853005     0.721058  (1308, 506, 269, 1561)   10.185824  \n",
       "15    0.781010     0.796175     0.765711  (1389, 425, 373, 1457)   17.614181  \n",
       "6     0.778540     0.863388     0.692944  (1257, 557, 250, 1580)   19.333888  \n",
       "2     0.777168     0.778689     0.775634  (1407, 407, 405, 1425)  204.612410  \n",
       "10    0.773875     0.772678     0.775083  (1406, 408, 416, 1414)  432.460099  \n",
       "11    0.772503     0.781421     0.763506  (1385, 429, 400, 1430)  207.255031  \n",
       "13    0.767838     0.820765     0.714443  (1296, 518, 328, 1502)  526.747730  \n",
       "4     0.766740     0.833333     0.699559  (1269, 545, 305, 1525)  555.867576  \n",
       "12    0.765917     0.757377     0.774531  (1405, 409, 444, 1386)  301.746136  \n",
       "3     0.761526     0.748087     0.775083  (1406, 408, 461, 1369)  296.348172  \n",
       "1     0.760154     0.740984     0.779493  (1414, 400, 474, 1356)  330.546137  \n",
       "16    0.753019     0.695082     0.811466  (1472, 342, 558, 1272)    9.061816  \n",
       "7     0.749726     0.692896     0.807056  (1464, 350, 562, 1268)   10.095613  \n",
       "17    0.732986     0.740437     0.725469  (1316, 498, 475, 1355)    6.832202  \n",
       "9     0.722558     0.942077     0.501103   (909, 905, 106, 1724)   43.186298  \n",
       "0     0.720088     0.940437     0.497795   (903, 911, 109, 1721)   43.944631  \n",
       "8     0.705543     0.831694     0.578280  (1049, 765, 308, 1522)    6.829363  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.sort_values(by='test_score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "We will use the scores gathered here to select our model in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
