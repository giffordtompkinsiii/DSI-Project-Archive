{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/vegan-logo-resized.png\" style=\"float: right; margin: 10px;\">\n",
    "\n",
    "# Data Collection\n",
    "\n",
    "Author: Gifford Tompkins\n",
    "\n",
    "---\n",
    "\n",
    "Project 03 | Notebook 0 of 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "The objective of this notebook is to scrape a substantial body of posts from the two subreddits [r/Vegan](https://www.reddit.com/r/vegan/) and [r/Vegetarian](https://www.reddit.com/r/vegetarian/) and to save them into a CSV file for preprocessing and data cleaning, to be completed in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Collection\" data-toc-modified-id=\"Data-Collection-1\">Data Collection</a></span></li><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-2\">Objective</a></span></li><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-3\">Import Libraries</a></span></li><li><span><a href=\"#Collecting-Data-Using-pushshift.io\" data-toc-modified-id=\"Collecting-Data-Using-pushshift.io-4\">Collecting Data Using <em>pushshift.io</em></a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-the-corpus\" data-toc-modified-id=\"Creating-the-corpus-4.1\">Creating the corpus</a></span></li><li><span><a href=\"#Confirm-the-corpus\" data-toc-modified-id=\"Confirm-the-corpus-4.2\">Confirm the corpus</a></span></li></ul></li><li><span><a href=\"#Summary-and-Conclusion\" data-toc-modified-id=\"Summary-and-Conclusion-5\">Summary and Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions saved in repository\n",
    "from project_functions.data_collection import get_data_json, create_doc, append_to_corpus    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data Using _pushshift.io_\n",
    "\n",
    "We will be collecting posts (or documents, as they will be referred to in this notebook) through the [pushshift.io API](https://pushshift.io/). For that we will submit a base `url` with a dictionary of parameters. One of those parameters being the name of the subbreddit, either `r/Vegan` or `r/Vegetarian`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/submission/search'\n",
    "subreddits = ['Vegetarian','Vegan']  \n",
    "corpus_csv = '../data/corpus.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the corpus\n",
    "We will create an empty dataframe to be saved as our `api_data.csv` that we can append each new batch of documents onto. Since we will be doing multiple pulls and hopefully getting up to 20,000 rows of data, we would like to save our data as we go in the event that our connection breaks or our function or program times out.\n",
    "\n",
    "For custom functions used in this section, see the [data collection code](./project_functions/data_collection.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/api_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4944601762dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create empty dataframe to store our corpus as a csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'vegan'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv-project-03/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[1;32m   3165\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m         )\n\u001b[0;32m-> 3167\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv-project-03/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             f, handles = get_handle(\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv-project-03/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/api_data.csv'"
     ]
    }
   ],
   "source": [
    "# Create empty dataframe to store our corpus as a csv file\n",
    "df = pd.DataFrame(columns=['title','selftext','vegan'])\n",
    "df.to_csv(corpus_csv,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r/Vegetarian: Pull number 1\n",
      "https://api.pushshift.io/reddit/submission/search?subreddit=Vegetarian&size=500\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/api_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4adfbd5ce40c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Append the batch of documents to the corpus using the custom function append_to_corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mappend_to_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Update 'before' paramter in url_params before running next batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dsi/projects/DSI-Project-Archive/03_Keyword_Vegan/code/project_functions/data_collection.py\u001b[0m in \u001b[0;36mappend_to_corpus\u001b[0;34m(batch, corpus)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Concatenate new dataframe to corpus from csv.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mdf_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'vegan'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/api_data.csv'"
     ]
    }
   ],
   "source": [
    "# Loop through the two subreddits and pull the document batches with the pushift.io api.\n",
    "for i, sub in enumerate(subreddits):\n",
    "    \n",
    "    # Instantiate and define the url parameters dictionary.\n",
    "    url_params = {\n",
    "        'subreddit': sub,\n",
    "        'size': 500\n",
    "    }\n",
    "    \n",
    "    # Pull 20 batches of posts for each subreddit.\n",
    "    for count in range(20):\n",
    "        \n",
    "        # Pull data from json\n",
    "        posts = get_data_json(count, url, url_params)\n",
    "\n",
    "        # Instantiate the bath of documents as a list item\n",
    "        documents = []\n",
    "        \n",
    "        for post in posts:\n",
    "            \n",
    "            # Create a document from the post using the custom function create_doc\n",
    "            # Classify the post as Vegan post: 1 or a Vegetarian post: 0\n",
    "            document = create_doc(i, post)\n",
    "            \n",
    "            # Append the document onto the documents batch\n",
    "            documents.append(document)\n",
    "        \n",
    "        # Append the batch of documents to the corpus using the custom function append_to_corpus\n",
    "        append_to_corpus(documents, corpus_csv)\n",
    "        \n",
    "        # Update 'before' paramter in url_params before running next batch.\n",
    "        url_params['before'] = data[-1]['created_utc']\n",
    "\n",
    "        # Put a delay on our loop to slow down api usage\n",
    "        time.sleep(10)\n",
    "        \n",
    "    # Remove the before parameter so that it can be redefined in the next subreddit\n",
    "    url_params.pop('before')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm the corpus\n",
    "Confirm that the posts have been pulled and that the data looks appropriate. We will do more extensive data-cleaning and EDA in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/api_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>vegan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Lentil soup with sliced Beyond Meat sausage ta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Lentil soup with sliced Beyond Meat sausage ta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Recipes for thanksgiving?</td>\n",
       "      <td>Hi! My bf’s sister is hosting thanksgiving thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>What Happens IMMEDIATELY After You Die?? The T...</td>\n",
       "      <td>[https://www.youtube.com/watch?v=Dyvl\\_8SoAjc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Vegetarian Chili for a cold October evening</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Lentil soup with sliced Beyond Meat sausage ta...   \n",
       "1  Lentil soup with sliced Beyond Meat sausage ta...   \n",
       "2                          Recipes for thanksgiving?   \n",
       "3  What Happens IMMEDIATELY After You Die?? The T...   \n",
       "4        Vegetarian Chili for a cold October evening   \n",
       "\n",
       "                                            selftext  vegan  \n",
       "0                                                NaN      0  \n",
       "1                                                NaN      0  \n",
       "2  Hi! My bf’s sister is hosting thanksgiving thi...      0  \n",
       "3   [https://www.youtube.com/watch?v=Dyvl\\_8SoAjc...      0  \n",
       "4                                                NaN      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>vegan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20995</td>\n",
       "      <td>Curious if you guys think this is a step in th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20996</td>\n",
       "      <td>The truth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20997</td>\n",
       "      <td>I am sorry guys.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20998</td>\n",
       "      <td>I call this a “Sloppy Mel”- beyond meat, Jones...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20999</td>\n",
       "      <td>Can I suggest any Facebook users do one small ...</td>\n",
       "      <td>Okay this isn't real activism and it's okay to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "20995  Curious if you guys think this is a step in th...   \n",
       "20996                                          The truth   \n",
       "20997                                   I am sorry guys.   \n",
       "20998  I call this a “Sloppy Mel”- beyond meat, Jones...   \n",
       "20999  Can I suggest any Facebook users do one small ...   \n",
       "\n",
       "                                                selftext  vegan  \n",
       "20995                                                NaN      1  \n",
       "20996                                                NaN      1  \n",
       "20997                                                NaN      1  \n",
       "20998                                                NaN      1  \n",
       "20999  Okay this isn't real activism and it's okay to...      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10500\n",
       "0    10500\n",
       "Name: vegan, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['vegan'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       0.000\n",
       "selftext    0.571\n",
       "vegan       0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "> Appears to be some duplicates and the selftext column has selveral null values, but all items are stored properly in the [`corpus.csv`](./datasets/api_data.csv). The first posts are vegetarian while last posts are vegan and we have pulled equal numbers of posts from each subreddit.  \n",
    ">\n",
    "> The corpus looks ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Conclusion\n",
    "We have gathered a solid body of data for our NLP analysis. We did this through submitting batch request through the pushift.io API. We then stored the data of interest into a corpus that we saved as a CSV.\n",
    "\n",
    "In the next notebook we will clean our data and do some initial exploratory analysis. After that we will have a better sense of direction for building our Vegan classification model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
